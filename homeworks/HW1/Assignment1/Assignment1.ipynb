{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NcuNbmvUqvZw"
   },
   "source": [
    "# Assignment 1: Classifying Inlfuenza using VAPOR\n",
    "By: Alex Nails, Osher Lerner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XKWJOwOYszC_"
   },
   "source": [
    "## First off, Let's recap from Week 1's Lecture and provide some introduction to this assignment. \n",
    "We learned what it means to classify, the basics of the machine learning pipeline, and the importance of a model improving as it learns to the data. In this assignment you will learn about a structure called the De Bruijin Graph and engage in a classification assignment. If you're still a little confused on what classification is, we have provided a Linear Classifier Demo in this assignment called **LinearClassifierDemo.ipynb**.\n",
    "\n",
    "## Here is some background on the Flu\n",
    "Influenza viruses are enveloped, single-stranded, segmented negative-sense RNA viruses of the family Orthomyxoviridae. Influenza A and B have 8 genome segments encoding major structural and nonstructural proteins, with major antigenic recognition sites within the two spike proteins haemagglutinin (HA) and neuraminidase (NA). Influenza replication in the host cell occurs by means of a viral encoded polymerase that lacks proof-reading capability, leading to frequent point mutations. Accumulation of these point mutations within the antigenic recognition sites of HA and NA can result in host immune evasion, thereby causing annual seasonal epidemics. Current estimates suggest that seasonal influenza A and B cause 4-5 million severe infections in humans with approximately 291,000 to 645,000 deaths per year globally. Whilst influenza B remains largely a human pathogen, influenza type A is a zoonotic virus infecting a\n",
    "wide range of avian and other non-human species. To date 18 haemagglutinin and 11 neuraminidase types have been recognised, with a reservoir for the majority within birds. These viruses have the\n",
    "capability to reassort leading to the emergence of new strain. Should these include new HA and NA proteins, pandemic viruses can emerge that completely evade the host response leading to global epidemics of often high morbidity and mortality in both non-human and human hosts.\n",
    "\n",
    "## Here is some background on past methods.\n",
    "Whole genome sequencing (WGS) has been used to study the influenza virus genome for over a decade, and is as an important tool in research. Two important data  resources exist to this end; the NCBI Influenza Virus Resource (NIVR), and the Global Initiative on Sharing All Influenza Data (GISAID), wherein over a hundred thousand influenza genome segment sequences can be found from isolates sequenced across the globe. Bioinformatics pipelines have begun to be developed for efficient processing of this data. Despite the increasing application of Next-Generation Sequencing (NGS) to influenza, the pitfalls associated with current mapping approaches have not been explored in depth. Due to diversity of influenza genome sequences, past methods routinely result in a large number of unmapped reads. In turn, this can potentially result in data loss and bias in sequences that are subsequently recovered, analyzed, and submitted to public databases. This has been previously noted in study of human immunodeficiency virus (HIV). Whilst alternatives, such as read classification by mapping to a large database of influenza sequences and subsequent processes can help to resolve this issue, such pipelines are often complex, slow, and require expertise that is not necessarily available in routine surveillance. Secondly, even if bioinformatics pipelines are chosen carefully, sequences of\n",
    "zoonotic (animals) origin may fail to be identified, resulting in a dataset that appears to be low coverage, missing segments, or missing potential future pandemic reassortments.\n",
    "\n",
    "## Here is the goal of the assignment\n",
    "We aim to show that this problem can be resolved by classification of isolates from reads of genetic data prior to WGS/NGS analysis by directly querying a De Bruijn graph (DBG). To further develop a simple method for querying short influenza genome sequences, we leverage the large number of publicly available influenza segment sequences, creating this VAPOR classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s0hn5AoUs2U7"
   },
   "source": [
    "### To start coding, first we must import the necessary libaries and assemble our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mwbrQqXktI96"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import gzip\n",
    "import random\n",
    "#this function sets numpy related objects to be fully printed no matter their size\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_2aemTG9uGkJ"
   },
   "source": [
    "As stated in the introduction to the assignment, the main structure behind this classifier utilizes the properties of a De Bruijin Graph. They utilize **kmers**, a set of k long substrings relative to some larger string of genetic data. For example, if the string was 'ATG' and k = 2, you would have 'AT' and 'TG' as kmers. Using kmers, we will build our De Bruijin Graph. \n",
    "\n",
    "These kmers are utilized by the graph to build nodes that are subsets of a set of kmers. Each node will have a corresponding weight which relates to how heavily the DNA subsequence is found, allowing us to update the weights and then classify once weights have been sufficiently trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='dbg.jpg') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look over these helpers functions to make sure you have a rough idea of what they do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zd_dJ6dZwYb4"
   },
   "outputs": [],
   "source": [
    "def kmers2str(kmers):\n",
    "    \"\"\" Takes a set off kmers and extracts their string \"\"\"\n",
    "    s = kmers[0]    \n",
    "    for k in kmers[1:]:\n",
    "        s += k[-1]\n",
    "    return s\n",
    "\n",
    "def get_kmers(strings, k):\n",
    "    \"\"\" Takes strings and returns a set of kmers \"\"\"\n",
    "    kmers = []\n",
    "    for string in strings:\n",
    "        kmers.append([string[i:i+k] for i in range(len(string)-k+1)])\n",
    "    return kmers\n",
    "\n",
    "def get_kmers_set(strings, k):\n",
    "    \"\"\" Takes strings and returns a set of kmers \"\"\"\n",
    "    kmers = set()\n",
    "    for string in strings:\n",
    "        for kmer in [string[i:i+k] for i in range(len(string)-k+1)]:\n",
    "            kmers.add(kmer)\n",
    "    return kmers\n",
    "\n",
    "def rev_comp(read):\n",
    "    \"\"\" Basic (slow) reverse complement of genetic pairs as well as formatting \"\"\"\n",
    "    read = read.replace(\"T\", \"a\")\n",
    "    read = read.replace(\"A\", \"t\")\n",
    "    read = read.replace(\"C\", \"g\")\n",
    "    read = read.replace(\"G\", \"c\")\n",
    "    return read.upper()[::-1]\n",
    "\n",
    "def parse_and_prefilter(fqs, dbkmers, threshold, k):\n",
    "    \"\"\" Parses fastq files (genetic sequence) fqs, and filters them \"\"\"\n",
    "    nraw = 0\n",
    "    reads = []\n",
    "    c = 0\n",
    "    with open(fqs, 'r') as file:\n",
    "        for line in file:\n",
    "            if c == 1:\n",
    "                nraw += 1\n",
    "                stripped = line.strip()\n",
    "                ktotal = int(len(stripped)/k)\n",
    "                # Don't allow Ns in read\n",
    "                # Don't allow reads < k\n",
    "                rev = rev_comp(stripped)\n",
    "                for tmpseq in [stripped, rev]: \n",
    "                    kcount = 0\n",
    "                    if \"N\" not in tmpseq and len(tmpseq) >= k:\n",
    "                        for i in range(0, len(tmpseq)-k+1, k):\n",
    "                            if tmpseq[i:i+k] in dbkmers:\n",
    "                                kcount += 1\n",
    "                                if kcount/ktotal > threshold:\n",
    "                                    reads.append(tmpseq)\n",
    "                                    # As soon as our threshold is exceeded, break\n",
    "                                    break\n",
    "            c += 1                  \n",
    "            if c == 4:\n",
    "                c = 0\n",
    "    return reads, nraw\n",
    "\n",
    "def parse_fasta_uniq(fasta, filter_Ns=True):\n",
    "    \"\"\" Gets unique sequences from a fasta (another format for text-based genetic data), with filtering of Ns\"\"\"\n",
    "    tmph = \"\"\n",
    "    tmps = \"\"\n",
    "    hs = []\n",
    "    ss = []\n",
    "    sseen = set()\n",
    "    with open(fasta) as f:\n",
    "        for li, line in enumerate(f):\n",
    "            l = line.strip()\n",
    "            if not l.startswith(\">\"):\n",
    "                l = l.upper()\n",
    "            if len(l) == 0:\n",
    "                continue\n",
    "            elif l[0] == \">\":\n",
    "                if tmps not in sseen and li > 0:\n",
    "                    if ((filter_Ns == True) and \"N\" not in tmps) or filter_Ns == False:\n",
    "                        hs.append(tmph)\n",
    "                        ss.append(tmps) \n",
    "                        sseen.add(tmps)\n",
    "                tmph = l\n",
    "                tmps = \"\"\n",
    "            else:\n",
    "                tmps += l\n",
    "    hs.append(tmph)\n",
    "    ss.append(tmps)\n",
    "    return hs, ss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JYCg9WxTwYyh"
   },
   "source": [
    "### Since we are finished with building the helper functions that have their various utilties, we will now move on to the classifier. \n",
    "\n",
    "### There are multiple functions here, marked with a TODO flag. Some come with hints, others do not. The classify function itself is done for you and the goal of this is for you to get familiar, so your work is more OOP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h0EZAnwCqvvj"
   },
   "outputs": [],
   "source": [
    "class SearchResult():\n",
    "    \"\"\"\n",
    "    Class to hold search results and methods\n",
    "    Used for debugging\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # these are explained in seed :)\n",
    "        self.raw_weight_array = []\n",
    "        self.kmer_cov = 0\n",
    "        \n",
    "        #ignore this\n",
    "        self.filled_array = []\n",
    "\n",
    "class wDBG():\n",
    "    \"\"\" Basic DBG with associated edge weights \"\"\"\n",
    "    def __init__(self, strings, k):\n",
    "        \"\"\" Initialized with strings, k, and reference kmers \"\"\"\n",
    "        # Only explicitly store nodes\n",
    "        self.nodes = {}\n",
    "        self.k = k\n",
    "        self._build(strings)\n",
    "        self.caching = True\n",
    "        self.path_cache = {}\n",
    "        self.max_trim_size = self.k+1\n",
    "\n",
    "    def _build(self, strings):\n",
    "        for si, string in enumerate(strings):\n",
    "            kmers = [string[i:i+self.k] for i in range(len(string)-self.k+1)]\n",
    "            for kmer in kmers:\n",
    "                if kmer in self.nodes:\n",
    "                    self.nodes[kmer] += 1\n",
    "                else:\n",
    "                    self.nodes[kmer] = 1\n",
    "\n",
    "    def cull_low(self, min_cov=5):\n",
    "        \"\"\"\n",
    "        Culls kmers with a coverage less than min_cov\n",
    "        \"\"\"\n",
    "        keyvals = [(k, v) for k, v in self.nodes.items()]\n",
    "        for key, val in keyvals:\n",
    "            if val <= min_cov:\n",
    "                del self.nodes[key]            \n",
    "\n",
    "    def mask_against_bridge(self, query, bridge, gapl):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            self: this is an object\n",
    "            query: string that represents a subsequence\n",
    "            bridge: string that represnts a different subsequence\n",
    "            gapl: int that represents relative positions in larger sequence\n",
    "            \n",
    "        Returns:\n",
    "            mask: a list of indices in the larger subsequence that are found\n",
    "                  by differences in query and bridge, while accounting for relative\n",
    "                  position\n",
    "        \n",
    "        Takes two strings, query and bridge, returning indices in a the list provided \n",
    "        (mask) where they differ while taking their relative position \n",
    "        into account by using gapl.\n",
    "        \n",
    "        Example: an index in an 8 character string could be 0-7, however,\n",
    "        since this function is used from different substrings you have to account\n",
    "        for where it is in the overall sequence. We use gapl, the offset, to do that\n",
    "        by adding to the index where there is a mismatch.\n",
    "        \"\"\"\n",
    "        mask = []\n",
    "        #TODO: CODE HERE\n",
    "        return mask\n",
    "\n",
    "    def extend_bridge(self, kmer, n, direction, debug=True):\n",
    "        \"\"\"\n",
    "        Walks along the wDBG n positions\n",
    "        making heuristic locally optimal decisions\n",
    "        at branches. Returns string, score array\n",
    "        \"\"\"\n",
    "        # First check the cache\n",
    "        if self.caching == True:\n",
    "            if (kmer, n, direction) in self.path_cache:\n",
    "                return self.path_cache[(kmer, n, direction)]\n",
    "        string = kmer\n",
    "        scorearr = np.zeros(n)\n",
    "        if direction == 1:\n",
    "            si = 0\n",
    "        else:\n",
    "            si = -1\n",
    "        while len(string) < n+self.k:\n",
    "            if direction == 1:\n",
    "                poss_nodes = [string[-self.k+1:] + b for b in \"ATCG\"]\n",
    "            elif direction == -1:\n",
    "                poss_nodes = [b + string[:self.k-1] for b in \"ATCG\"]\n",
    "            max_score = 0\n",
    "            max_base = None\n",
    "            for pe in poss_nodes:\n",
    "                if pe in self.nodes:\n",
    "                    tmpscore = self.nodes[pe]\n",
    "                    if tmpscore > max_score:\n",
    "                        max_score = tmpscore\n",
    "                        if direction == 1:\n",
    "                            max_base = pe[-1]\n",
    "                        elif direction == -1:\n",
    "                            max_base = pe[0]\n",
    "            if max_base != None:\n",
    "                scorearr[si] = max_score\n",
    "                if direction == 1:\n",
    "                    si += 1\n",
    "                    string += max_base\n",
    "                elif direction == -1:\n",
    "                    si -= 1\n",
    "                    string = max_base + string\n",
    "            else:\n",
    "                break\n",
    "        if direction == 1:\n",
    "            string = string[len(kmer):]\n",
    "            string += \"X\"*(n-len(string))\n",
    "        elif direction == -1:\n",
    "            string = string[:-len(kmer)]\n",
    "            string = \"X\"*(n-len(string)) + string\n",
    "        if self.caching == True:\n",
    "            self.path_cache[(kmer, n, direction)] = (string, scorearr)\n",
    "        return string, scorearr\n",
    "\n",
    "    def get_raw_weight_array(self, kmers):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            kmers: a (very) long list of k-long subsequences of genetic data\n",
    "            \n",
    "        Returns: \n",
    "            array: an array of weights that is given to us by searching\n",
    "                   through the nodes of our DBG, seeing if a kmer is present. \n",
    "                   If kmer is not present in nodes, it is 0 (done for you with np.zero).\n",
    "                   \n",
    "        hint 1: you know you'll need iteration and using two variables (index, value) will be helpful\n",
    "        hint 2: since your array is already created, all you have to do is index into\n",
    "              self.nodes using a specifc kmer and index into the premade array itself \n",
    "              using an iter value\n",
    "        \"\"\"\n",
    "        array = np.zeros(len(kmers))\n",
    "        #TODO: CODE HERE\n",
    "        return array\n",
    "    \n",
    "    def get_weight_array_gaps(self, array):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            array: your weight array you just built, you should know what this is\n",
    "                   right from the function above\n",
    "            \n",
    "        Returns:\n",
    "            gaps: a list of nested lists which are [gapl, gapr] pairs,\n",
    "                  which specify the interval of which they are 0's in the weight array. \n",
    "                  this is done by the index of where the gap starts and ends. should look like\n",
    "                  [[gapl, gapr],...,[newgapl, newgapr]], where .append() might be useful\n",
    "                \n",
    "        hint 1: you know you'll need iteration and using two variables (index, value) will be helpful\n",
    "        hint 2: the boolean flag in_gap is if the sequence ends with a 0,\n",
    "                so feel free to ignore it and structure the code differently.\n",
    "                \n",
    "        \"\"\"\n",
    "        in_gap = False\n",
    "        gaps = []\n",
    "        gapl = 0\n",
    "        gapr = 0\n",
    "        #TODO: CODE HERE\n",
    "        return gaps    \n",
    "\n",
    "    def deque_score_bases(self, array):\n",
    "        \"\"\" \n",
    "        This is a version of the sliding window maximum problem.\n",
    "        \n",
    "        Parameters:\n",
    "            array: this is your raw_weight array concatenated with \n",
    "                   an array of 0's that is k-1 long, since the deque itself\n",
    "                   is init'ed with a 0.\n",
    "                   \n",
    "        Returns:\n",
    "            local_maxima: an array of local maxima indexes that are listed for every entry.\n",
    "            This means that if the max for the past 3 iterations has been index 15, the deque\n",
    "            has kept 15 and added it to local maxima 3 times resulting in [15, 15, 15].\n",
    "                   \n",
    "        The goal of this is to find the local max weight in a subset of the array\n",
    "        \n",
    "        hint1: subtraction self.k from the iteration you're on could check whether the the deque's\n",
    "               lowest index was still in the k-long window\n",
    "            \n",
    "        hint2: checking to see if the highest index in deque was still the max value in the array window\n",
    "               and seeing if you need to replace is necessary as well.\n",
    "        \n",
    "        Look at https://www.geeksforgeeks.org/sliding-window-maximum-maximum-of-all-subarrays-of-size-k/\n",
    "        if you are still struggling to understand this. (method 3)\n",
    "        \"\"\" \n",
    "        local_maxima = np.zeros(len(array))\n",
    "        deq = deque(maxlen=self.k)\n",
    "        deq.append(0)\n",
    "        #TODO: YOUR CODE HERE\n",
    "        local_maxima[-1] = array[deq[0]]\n",
    "        return local_maxima\n",
    "\n",
    "    def get_suboptimal_branches(self, kmers):\n",
    "        \"\"\"\n",
    "        Walks backward along an array of kmers until a\n",
    "        step is detected that is suboptimal\n",
    "        returns positions of these branches\n",
    "        \"\"\"\n",
    "        suboptimal_branches = set()\n",
    "        bases = list(\"ATCG\")\n",
    "        for ki, kmer in enumerate(kmers):\n",
    "            if kmer in self.nodes:\n",
    "                score = self.nodes[kmer]\n",
    "                alts = [kmer[:-1] + b for b in bases]\n",
    "                altscores = [self.nodes[amer] for amer in alts if amer in self.nodes]\n",
    "                if altscores != []:\n",
    "                    if score < max(altscores):\n",
    "                        suboptimal_branches.add(ki)\n",
    "        return suboptimal_branches \n",
    "\n",
    "    def expand_gaps(self, gaps, suboptimal_branches, max_gapr):        \n",
    "        \"\"\" \n",
    "        Acts in place to expand gaps to suboptimal branch positions\n",
    "        takes the most distant sub branch within self.max_trim_size\n",
    "        \"\"\"\n",
    "        for gapi, gap in enumerate(gaps):\n",
    "            gapl, gapr = gap\n",
    "            for li in range(max(0, gapl-self.max_trim_size), gapl):\n",
    "                if li in suboptimal_branches:\n",
    "                    gaps[gapi][0] = li\n",
    "                    break\n",
    "            for ri in range(min(gapr + self.max_trim_size, max_gapr), gapr, -1):\n",
    "                if ri in suboptimal_branches:\n",
    "                    gaps[gapi][1] = ri\n",
    "                    break\n",
    "\n",
    "    def seed(self, kmers):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            kmers: a (very) long list of k-long sequences\n",
    "            \n",
    "        Returns:\n",
    "            the updated SearchResult object, with new kmer_cov\n",
    "            and get_raw_weight_array data.\n",
    "            \n",
    "        The DBG is starting its classification with this function \n",
    "        by obtaining the relevant data for the iteration the classify function\n",
    "        is on. The two fields that are updated in sr are kmer_cov and the\n",
    "        weight array. kmer_cov is the proportion of the number nonzero weights\n",
    "        to the entire set of weights in this iteration. the raw_weight_array\n",
    "        is already made, you just have to GET it. After updating the object's\n",
    "        instances, return it.\n",
    "        \n",
    "        hint: np.count_nonzero is pretty helpful for kmer_cov\n",
    "        \"\"\"\n",
    "        sr = SearchResult()\n",
    "        # TODO: YOUR CODE HERE\n",
    "        return sr\n",
    "\n",
    "    def complete_query(self, sr, kmers=None, string=None, debug=False):\n",
    "        if kmers == None:\n",
    "            kmers = get_kmers([string], self.k)[0]\n",
    "        kmer_cov = sr.kmer_cov\n",
    "        raw_weight_array = sr.raw_weight_array\n",
    "        # Get the gaps\n",
    "        gaps = self.get_weight_array_gaps(raw_weight_array)\n",
    "        # Get the suboptimal branches\n",
    "        sub = self.get_suboptimal_branches(kmers)\n",
    "        self.expand_gaps(gaps, sub, len(kmers))\n",
    "        filled_weight_array = raw_weight_array\n",
    "        all_masks = []\n",
    "        for gapl, gapr in gaps:\n",
    "            if gapl != 0 and gapr != len(kmers):\n",
    "                gapstring = kmers2str(kmers[gapl:gapr])[self.k-1:]\n",
    "                bridge, bridge_scores = self.extend_bridge(kmers[gapl-1], gapr-gapl, 1, debug)\n",
    "                bridge_rev, bridge_scores_rev = self.extend_bridge(kmers[gapr], gapr-gapl, -1, debug)\n",
    "                gapstring_rev = kmers2str(kmers[gapl:gapr])[:-self.k+1]\n",
    "                if sum(bridge_scores_rev) > sum(bridge_scores):\n",
    "                    mask = self.mask_against_bridge(gapstring_rev, bridge_rev, gapl)\n",
    "                    filled_weight_array[gapl:gapr] = bridge_scores_rev\n",
    "                else:\n",
    "                    mask = self.mask_against_bridge(gapstring, bridge, gapl)\n",
    "                    filled_weight_array[gapl:gapr] = bridge_scores\n",
    "\n",
    "            elif gapr != len(kmers) and gapl == 0:\n",
    "                gapstring = kmers2str(kmers[gapl:gapr])[self.k-1:]\n",
    "                bridge, bridge_scores = self.extend_bridge(kmers[gapr], gapr-gapl, -1, debug)\n",
    "                mask = self.mask_against_bridge(gapstring, bridge, gapl)\n",
    "                filled_weight_array[gapl:gapr] = bridge_scores\n",
    "\n",
    "            elif gapl > 0 and gapr == len(kmers):\n",
    "                gapstring = kmers2str(kmers[gapl:gapr])[self.k-1:]\n",
    "                bridge, bridge_scores = self.extend_bridge(kmers[gapl-1], gapr-gapl, 1, debug)\n",
    "                mask = self.mask_against_bridge(gapstring, bridge, gapl)\n",
    "                filled_weight_array[gapl:gapr] = bridge_scores\n",
    "            all_masks += mask\n",
    "        # Deque score\n",
    "        filled_weight_array = np.concatenate((filled_weight_array, np.zeros(self.k-1)))\n",
    "        filled_deque_array = self.deque_score_bases(filled_weight_array)\n",
    "        for maski in all_masks:\n",
    "            filled_deque_array[maski] = 0\n",
    "        # Sum, also get estimated pid\n",
    "        nonzeros = [i for i in filled_deque_array if i > 0]\n",
    "        est_pid = len(nonzeros)/len(filled_deque_array)\n",
    "        sr.est_pid = est_pid\n",
    "        score = sum(nonzeros)\n",
    "        sr.score = score\n",
    "        return sr\n",
    "    \n",
    "    \n",
    "    def classify(self, seqs, seqsh, min_kmer_prop, top_seed_frac, debug_query=None, low_mem=False):\n",
    "        \"\"\"\n",
    "        Queries a set of sequences seqs, with headers seqsh,\n",
    "        parameter min_kmer_prop\n",
    "        and if debugging, a debug query\n",
    "        \"\"\"\n",
    "        seeds = []\n",
    "        for si, seq in enumerate(seqs):\n",
    "            kmers = [seq[i:i+self.k] for i in range(len(seq)-self.k+1)]\n",
    "            seed = self.seed(kmers)\n",
    "            seed.index = si\n",
    "            if seed.kmer_cov > min_kmer_prop:\n",
    "                seed.kmers = kmers\n",
    "                seeds.append(seed)\n",
    "\n",
    "        topseeds = sorted(seeds, key=lambda x:x.kmer_cov, reverse=True)[:int(np.ceil(top_seed_frac*len(seqs)))]\n",
    "\n",
    "        scores = []\n",
    "        for seed in topseeds:\n",
    "            sr = self.complete_query(seed, seed.kmers, None)\n",
    "            scores.append(sr)\n",
    "\n",
    "        # Sort the results\n",
    "        results = sorted(scores, key = lambda x: x.score * x.est_pid, reverse=True)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A-V1vY0tw3X-"
   },
   "source": [
    "Now that the model is finished we must train and test it. Since genetic sequences are unique and De Bruijin relies on mapping, this all looks like it happens behind the scenes since the weights are updated without us ever looking at the weights at indiviudal timesteps. Usually, a large dataset would be split and the model would be saved after training on some unique subset of that dataset and be tested on a different unique subset as validation where we'd be able to see progress over time using a tool (tensorboard or similiar). What makes this specific classifier so amazing is that VAPOR does not require any preprocessing, it is the same as taking a swab of DNA, digitally encoding it, and putting it into a classifier. In comparison, BLAST (basic local alignment seearch tool) requires pre-processing and doesn't deal with mutations nor physical imperfections of isolates that well, so they must be analyzed and \"fixed\" (Using other algorithms to reformat mutated DNA, based of a likeliness of relation to close genetic strings, an application of genetic phylogenesis/homology). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    sys.stderr.write(\"Loading database sequences\\n\")\n",
    "    seqsh, seqs = parse_fasta_uniq('PATH HERE TO FA')\n",
    "    sys.stderr.write(\"Got %d unique sequences\\n\" % len(seqs))\n",
    "\n",
    "    # Get database kmers for filtering\n",
    "    sys.stderr.write(\"Getting database kmers\\n\")\n",
    "    dbkmersset = get_kmers_set(seqs, 21)\n",
    "    sys.stderr.write(\"Got %d database kmers\\n\" % len(dbkmersset))\n",
    "\n",
    "    # Parse and pre-filter reads\n",
    "    sys.stderr.write(\"Filtering reads\\n\")\n",
    "    reads, nrawreads = parse_and_prefilter('PATH HERE TO FA', dbkmersset, 0.2, 21)\n",
    "    nreads = len(reads)\n",
    "    sys.stderr.write(\"%d of %d reads survived\\n\" % (nreads,nrawreads))\n",
    "\n",
    "    # Check there are still sequences remaining\n",
    "    if nreads == 0:\n",
    "        sys.stderr.write(\"Exiting. No virus found in your sequences. Try a lower filtering threshold, or a bigger set of references.\\n\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Build the wDBG from reads\n",
    "    sys.stderr.write(\"Building wDBG\\n\")\n",
    "    wdbg = wDBG(reads, 21)\n",
    "    sys.stderr.write(\"Got %d wdbg kmers\\n\" % len(wdbg.nodes))\n",
    "    \n",
    "    # Cull low coverage\n",
    "    sys.stderr.write(\"Culling kmers with coverage under %d \\n\" % 5)\n",
    "    wdbg.cull_low(5)\n",
    "    sys.stderr.write(\"%d kmers remaining\\n\" % len(wdbg.nodes))\n",
    "\n",
    "    if len(wdbg.nodes) == 0:\n",
    "        sys.stderr.write(\"Zero kmers remaining after culling! Try a lower coverage cutoff -c. \\n\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Ask the wdbg to classify\n",
    "    sys.stderr.write(\"Classifying\\n\")\n",
    "    path_results = wdbg.classify(seqs, seqsh, 0.1, 0.2, None, False)\n",
    "    results = path_results[:1]\n",
    "    results = [(sr.index, sr.est_pid, sr.score) for sr in results if sr.score != -1]\n",
    "    if len(results) == 0:\n",
    "        sys.stderr.write(\"No hits. Try a lower -m threshold\\n\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Output results\n",
    "    for c, est_pid, score in results:\n",
    "        if score != -1:\n",
    "            slen = len(seqs[c])\n",
    "            mean = str(score/slen)\n",
    "            print(str(est_pid) + \"\\t\" + str(score)+\"\\t\"+str(slen)+\"\\t\" +str(mean) + \"\\t\"+ str(nreads) + \"\\t\"+seqsh[c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After running the program and getting the flu virus classified and getting its' information, how does the accuracy score match up with BLAST?**\n",
    "\n",
    "`Your Answer Here`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "09E7GgU9AQPU",
    "outputId": "82df4aa7-6e2f-478a-aa94-31e9029b7214",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "blast_mean_res = 99.48\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oLO45dHFrTGI"
   },
   "source": [
    "## After reading the paper for this week, \"How to Read a Paper\" and completing this assignment, please go to https://forms.gle/SyFb6TtigBuTkJLG7 and submit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oLO45dHFrTGI"
   },
   "source": [
    "## Congratulations on finishing Assignment 1! If you have any further questions about the assignment or how a classifier works, feel free to email or contact us on Piazza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-HWptaXk2-pf"
   },
   "source": [
    "Thank you Joel Southgate and the Connor Lab for your contributions and work done on this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
