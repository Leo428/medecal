{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Identifying Unreported Cases of COVID-19\n",
    "\n",
    "This assignment was adapted from [this paper](https://arxiv.org/abs/2006.02127), titled _Data-driven Identification of Number of Unreported Cases for COVID-19: Bounds and Limitations_. Feel free to read the paper if you like, but you will not need to know anything in the paper that is not explained here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Goal\n",
    "\n",
    "The goal of this assignment is to come up with a reasonable, data-driven estimate of how many people have COVID-19. You will have access to the data with the number of _reported_ cases. But the reported numbers are, of course, under-estimates because there are people who go unreported.\n",
    "\n",
    "It seems reasonable to assume that the number of people who have it is proportional to the number reported. \n",
    "\n",
    "$$\n",
    "\\text{Number infected} = k \\times \\text{Number Reported}, \\qquad k > 1\n",
    "$$\n",
    "\n",
    "Therefore, we just need to determine how many infected people a single case represents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Recall that epidemiological models can have compartments representing groups of people with transition rates or probabilities between the compartments. In this assignment, we will have an _S_ (susceptible), an _I_ (infected), and an _R_ (reported) compartment. There's also a compartment for immune / isolated people who are, naturally, not connected to any other compartment. Let's call the total number of people in the population $N$.\n",
    "\n",
    "|Variable|Meaning|\n",
    "|--------|-------|\n",
    "|$S_t$|No. susceptible people at time $t$|\n",
    "|$I_t$|No. infected people at time $t$|\n",
    "|$R_t$|No. reported people at time $t$|\n",
    "|$N$|Total people in population|\n",
    "\n",
    "|Transition|Associated Rate|\n",
    "|----------|:---------------:|\n",
    "|$S \\to I$|Infection rate $\\beta$|\n",
    "|$I \\to R$|Reporting probability $\\gamma$|\n",
    "\n",
    "\n",
    "At some time that we'll call $t$, the number of infected people $I_t$ changes over time in the following way (this is just part of the model, so we will not justify it too much). We'll measure $t$ with days, so $t-7$ is the value a week before $t$.\n",
    "\n",
    "$$\n",
    "\\Delta I_t = \\frac{S_t}{N}\\beta \\left( I_{t} - I_{t-7} \\right).\n",
    "$$\n",
    "\n",
    "But we won't have access to the number infected people, so we need to rewrite this to be only in terms of reported numbers. I will wave my mathematical wand and tell you that\n",
    "\n",
    "\\begin{equation}\n",
    "\\Delta R_t = \\left(1 - \\frac{R_t}{\\gamma(1-\\rho)N} \\right)\\beta \\left( R_{t} - R_{t-7} \\right). \\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "In the equation above, $\\rho$ is the percent of people who are immune / isolated from COVID and therefore play no part in its spread. We have no information about who is immune or isolated, so the best we can do is absorb the constant and learn $\\bar\\gamma = \\gamma(1-\\rho)$ from the data. Observe that this makes the empirical value we find always an underestimate: $\\bar\\gamma \\leq \\gamma$.\n",
    "\n",
    "A derivation is included in section 3.1 of the paper if you're curious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory into practice\n",
    "\n",
    "Now it's your turn to start implementing these ideas! There are only a few big steps:\n",
    "\n",
    "1. Write code to do the computations according to the model equations.\n",
    "3. Use the case reports to find the parameters that best describe the data.\n",
    "4. Visualize / interpret our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(Xt, Rt, beta, gamma, N):\n",
    "    \"\"\"\n",
    "    Xt: an array of values [ Rt - R(t-7) ] for every time t\n",
    "    Rt: the number of reported cases at time t\n",
    "    beta: infection rate\n",
    "    gamma: reporting rate\n",
    "    \n",
    "    Return Delta Rt given data and parameters to model. \n",
    "    Basically implement equation 1. \n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "def diff_with_delay(x, J):\n",
    "    \"\"\"\n",
    "    Returns the differences in x over a window of size J.\n",
    "    x is an array representing data sampled at regular intervals.\n",
    "    Therefore if J = 1, you would return [ x[1] - x[0], x[2] - x[1], ... ]\n",
    "    And if J = 2, you would return [ x[2] - x[0], x[3] - x[1], ... ]\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "\n",
    "def least_squares_error(Xt, Rt, beta, gamma, N):\n",
    "    preds       = model(Xt, Rt, beta, gamma, N)\n",
    "    delta_Rt    = (Rt[1:] - Rt[:-1])[6:]\n",
    "    ### YOUR CODE HERE\n",
    "    # Take a look here https://pytorch.org/docs/stable/nn.html to find a suitable loss function.\n",
    "    # We want a function that is the sum of the squares of difference between our predictions and the given values, perhaps multiplied by a constant.\n",
    "    # You just need to make lossFn point to a variable in the torch package. Do not call the function here.\n",
    "    lossFn = None\n",
    "    \n",
    "    loss = lossFn(reduction='sum')( preds, delta_Rt )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to touch this cell. We are providing this for you.\n",
    "# Retrieved from https://stackoverflow.com/questions/40443020/matlabs-smooth-implementation-n-point-moving-average-in-numpy-python\n",
    "def smooth(a,WSZ):\n",
    "    # a: NumPy 1-D array containing the data to be smoothed\n",
    "    # WSZ: smoothing window size needs, which must be odd number,\n",
    "    # as in the original MATLAB implementation\n",
    "    out0 = np.convolve(a,np.ones(WSZ,dtype=int),'valid')/WSZ    \n",
    "    r = np.arange(1,WSZ-1,2)\n",
    "    start = np.cumsum(a[:WSZ-1])[::2]/r\n",
    "    stop = (np.cumsum(a[:-WSZ:-1])[::2]/r)[::-1]\n",
    "    return np.concatenate((  start , out0, stop  ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now consider the cases in New York State. The code below loads the time series data and puts the New York cases in the `data` variable. $N$ is the population of New York."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./time_series_covid19_confirmed_US.csv\")\n",
    "data = df[ df['Province_State'] == \"New York\" ].sum() \n",
    "\n",
    "### YOUR CODE HERE\n",
    "# What does N represent again? (We discussed this at the top of the notebook)\n",
    "# Use the U.S. Census Bureau rather than the first number Google shows.\n",
    "N = None\n",
    "\n",
    "# J is the window we will use. Leave as 7.\n",
    "J = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.keys() contains a lot of fields from the reports. \n",
    "# Select a slice of data.keys() so that it starts on the date 2/25/20 and ends on the data 7/6/20.\n",
    "\n",
    "### YOUR CODE HERE\n",
    "start = None\n",
    "end = None\n",
    "\n",
    "date_keys = data.keys()[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell just sets up variables useful later, but you don't need to do anything here.\n",
    "total_time_interval = [1, len(date_keys)]\n",
    "raw_total_cases = data[date_keys].to_numpy(dtype=np.float32)\n",
    "\n",
    "print(data[date_keys])\n",
    "\n",
    "Rt = smooth(data[start:end].to_numpy(dtype=np.float32), 7)\n",
    "\n",
    "training_interval = [ total_time_interval[1] - len(Rt) , total_time_interval[1] -1 ]\n",
    "model_interval = [training_interval[0] + 6, training_interval[1]]\n",
    "Xt = diff_with_delay(Rt, J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = torch.tensor(np.random.rand() * 10 , requires_grad=True)\n",
    "gamma = torch.tensor(np.random.rand(),  requires_grad=True)\n",
    "\n",
    "Rt = torch.tensor(Rt, dtype=torch.double)\n",
    "Xt = torch.tensor(Xt, dtype=torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will iteratively adjust the model parameters to minimize model error.\n",
    "\n",
    "# What two variables do we want to optimize over? Hint: what controls the future predictions?\n",
    "### YOUR CODE HERE\n",
    "params = [None, None]\n",
    "\n",
    "optimizer = torch.optim.Adam( params , lr=0.01 )\n",
    "\n",
    "steps = 4000\n",
    "for t in range(steps):\n",
    "    loss = least_squares_error(Xt, Rt, beta, gamma, N)\n",
    "    if t % 100 == 0:\n",
    "        print(\"After {} steps, the model's error = {}\".format(t, loss.item()))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(beta, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.diff(raw_total_cases), label = 'New Cases (Raw Data)')\n",
    "plt.plot(np.arange(*training_interval), np.diff(Rt), label = 'Training Data')\n",
    "preds = model(Xt, Rt, beta, gamma, N).detach().numpy()\n",
    "plt.plot(np.arange(*model_interval), preds, label = 'Model')\n",
    "plt.ylabel(\"Number of Cases\")\n",
    "plt.xlabel(\"Days from 2/25/20\")\n",
    "plt.legend()\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions / Exercises\n",
    "\n",
    "1. The plot above (hopefully) shows reasonable results from the model. How confident are you in the predictions that this model will make? How would you convince someone that this model's predictions are good? (What conclusion did we make about comparing models during Lecture 2?)\n",
    "2. What steps would you take to train a model and make a plot for the cases in Berkeley? Go ahead and try that. It should only require a few changes to this notebook.\n",
    "3. What assumption are we making about the values of $\\beta$ and $\\gamma$ over time?\n",
    "\n",
    "Put your answers in a markdown cell below.\n",
    "\n",
    "This week's paper is [Training Classifiers with Natural Language Explanations](https://arxiv.org/pdf/1805.03818.pdf). Give it a read and answer a few simple questions in the form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Question answers here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
